# HÆ°á»›ng Dáº«n Táº¡o Diagram cho Luá»“ng 1: Import Repos & Extract Features

## ğŸ“‹ Tá»•ng Quan Luá»“ng

**Luá»“ng 1: Import GitHub Repositories â†’ Extract Features**

Luá»“ng nÃ y bao gá»“m 3 giai Ä‘oáº¡n chÃ­nh:
1. **Import Phase**: NgÆ°á»i dÃ¹ng tÃ¬m kiáº¿m vÃ  chá»n repos tá»« GitHub
2. **Processing Phase**: Há»‡ thá»‘ng clone repo, fetch builds tá»« CI, lÆ°u vÃ o DB
3. **Feature Extraction Phase**: Há»‡ thá»‘ng extract features sá»­ dá»¥ng Hamilton DAG pipeline

---

## ğŸ¯ Use Case Diagram

### Actors (TÃ¡c nhÃ¢n)
1. **Developer** - NgÆ°á»i dÃ¹ng cá»§a há»‡ thá»‘ng
2. **System (Backend)** - Há»‡ thá»‘ng xá»­ lÃ½ async

### Use Cases

| ID | Use Case Name | Actor | Description |
|----|---|---|---|
| UC1 | Search GitHub Repos | Developer | TÃ¬m kiáº¿m repos trÃªn GitHub (public + private) qua GitHub API |
| UC2 | Select Repositories | Developer | Chá»n nhiá»u repos tá»« káº¿t quáº£ tÃ¬m kiáº¿m |
| UC3 | Configure Import Settings | Developer | Cáº¥u hÃ¬nh test frameworks, source languages, CI provider, build limits |
| UC4 | Import Repositories | Developer/System | Khá»Ÿi táº¡o import process |
| UC5 | Clone Repository | System | Clone/update git repository vÃ o local storage (BARE clone) |
| UC6 | Fetch Builds from CI | System | Fetch builds vÃ  logs tá»« CI provider (GitHub Actions, Travis, Jenkins, etc.) |
| UC7 | Extract Features | System | Extract features sá»­ dá»¥ng Hamilton DAG pipeline |
| UC8 | View Import Progress | Developer | Xem tiáº¿n Ä‘á»™ import qua WebSocket real-time updates |
| UC9 | View Build Metrics | Developer | Xem metrics cá»§a builds sau khi extract features |

### Má»‘i Quan Há»‡ (Relationships)
```
Developer â†’ UC1 (Search)
UC1 â†’ UC2 (Select)
UC2 â†’ UC3 (Configure)
UC3 â†’ UC4 (Import)

UC4 â†’ UC5 (Clone) [System]
UC5 â†’ UC6 (Fetch Builds) [System]
UC6 â†’ UC7 (Extract) [System]

UC4 â†’ UC8 (View Progress) [Developer]
UC7 â†’ UC8 (Update Progress)
UC7 â†’ UC9 (View Results) [Developer]
```

---

## ğŸŠ Activity Diagram vá»›i Swimlanes

### Swimlanes (5 cÃ¡i)

1. **UI / Frontend** (MÃ u xanh nháº¡t - #E1F5FE)
   - Giao diá»‡n nháº­p liá»‡u
   - Hiá»ƒn thá»‹ káº¿t quáº£
   - Cáº­p nháº­t real-time qua WebSocket

2. **API Layer** (MÃ u tÃ­m nháº¡t - #F3E5F5)
   - FastAPI routes
   - Authentication
   - Request validation
   - Database operations

3. **Celery Tasks** (MÃ u xanh lÃ¡ nháº¡t - #E8F5E9)
   - Async task orchestration
   - import_repo, clone_repo, fetch_and_save_builds, dispatch_processing, process_workflow_run

4. **Database** (MÃ u cam nháº¡t - #FFF3E0)
   - MongoDB operations
   - RawRepository
   - ModelRepoConfig
   - RawBuildRun
   - ModelTrainingBuild
   - Features storage

5. **External (GitHub/CI)** (MÃ u há»“ng nháº¡t - #FCE4EC)
   - GitHub API
   - Git operations
   - CI Providers API

### Chi Tiáº¿t CÃ¡c Giai Äoáº¡n

#### **Giai Äoáº¡n 1: SEARCH & SELECT (Synchronous)**

**Step 1-1: TÃ¬m kiáº¿m Repositories**
```
UI: User enters search query â†’ Click Search
API: POST /repos/search?q=query
  â†“ Get user's GitHub client
External: Query GitHub API
  â†“ Return public + private matches
API: Return RepoSuggestion[]
UI: Display search results
```

**Step 1-2: Chá»n & Cáº¥u HÃ¬nh**
```
UI: Select repositories from results
UI: Configure settings:
    - test_frameworks: ["pytest", "junit"]
    - source_languages: ["python", "java"]
    - ci_provider: "github_actions"
    - max_builds: 100
    - since_days: 180
    - only_with_logs: true
UI: Click Import button
```

#### **Giai Äoáº¡n 2: IMPORT (Mostly Async)**

**Step 2-1: Create Records & Queue Task**
```
API: POST /repos/import/bulk with payload[]
  â†“ For each repo:
    - Verify repo exists on GitHub
    - Create RawRepository
    - Create ModelRepoConfig (status=QUEUED)
API: Queue celery task: import_repo.delay()
API: Return RepoResponse[] with status=QUEUED
UI: Show confirmation, subscribe to WebSocket
```

**Step 2-2: Celery Chain - import_repo â†’ clone_repo â†’ fetch_and_save_builds â†’ dispatch_processing**

```
Celery: import_repo() triggered
  â””â”€ Update ModelRepoConfig status=IMPORTING
  â””â”€ Publish REPO_UPDATE event to Redis
UI: Receive update: "Importing..."
  
Celery: clone_repo() starts
  â”œâ”€ Check if repo_path exists in REPOS_DIR/repo_id/
  â”œâ”€ If exists: git fetch --all --prune
  â””â”€ If new: 
      â”œâ”€ Get installation token
      â””â”€ git clone --bare https://x-access-token:TOKEN@github.com/owner/repo.git
  â””â”€ Return repo_id
UI: Receive update: "Cloned successfully"

Celery: fetch_and_save_builds() starts
  â”œâ”€ Calculate since_dt = now - since_days
  â”œâ”€ Get CI provider instance
  â”œâ”€ ci_instance.fetch_builds(
  â”‚   full_name, 
  â”‚   since=since_dt,
  â”‚   limit=max_builds,
  â”‚   exclude_bots=True,
  â”‚   only_with_logs=only_with_logs
  â”‚ )
  â”œâ”€ For each build in results:
  â”‚   â”œâ”€ Find/Create RawBuildRun
  â”‚   â””â”€ Create ModelTrainingBuild (status=PENDING)
  â”œâ”€ Update ModelRepoConfig with build_count
  â””â”€ Return build_ids list
UI: Receive update: "Found N builds"

Celery: dispatch_processing() starts
  â”œâ”€ For batch in build_ids (batch_size=50):
  â”‚   â””â”€ Create group of tasks:
  â”‚       â””â”€ celery_app.signature("app.tasks.processing.process_workflow_run", 
  â”‚           args=[repo_id, build_id])
  â”‚       â””â”€ tasks.apply_async()
  â”œâ”€ Update ModelRepoConfig status=IMPORTED
  â””â”€ Publish "Import completed" event
UI: Mark repo as IMPORTED
```

#### **Giai Äoáº¡n 3: FEATURE EXTRACTION (Per Build)**

**Step 3-1: Process Workflow Run**
```
Celery: process_workflow_run(repo_id, build_id) triggered
  â”œâ”€ Publish BUILD_UPDATE: "in_progress"
  â””â”€ Fetch RawRepository, RawBuildRun, ModelRepoConfig
  
Celery: build_hamilton_inputs()
  â”œâ”€ repo_path = REPOS_DIR / repo_id
  â”œâ”€ Git history from repo_path
  â”œâ”€ Git worktree operations
  â””â”€ Prepare workflow_run, repo_config, repo inputs

Celery: HamiltonPipeline.run(
    git_history=...,
    git_worktree=...,
    repo=...,
    workflow_run=...,
    repo_config=...,
    github_client=None,
    features_filter=template.feature_names
  )

Hamilton DAG Execution:
  â”œâ”€ build_features extractor
  â”œâ”€ git_features extractor
  â”œâ”€ github_features extractor
  â”œâ”€ repo_features extractor
  â””â”€ log_features extractor (náº¿u cÃ³ logs)

Celery: Collect features â†’ Format for storage

DB: Update ModelTrainingBuild
  â”œâ”€ features: {...}
  â”œâ”€ extraction_status: COMPLETED
  â””â”€ Save extracted features

Celery: Publish BUILD_UPDATE: "completed"

UI: Receive WebSocket update
  â”œâ”€ Show build extraction status
  â””â”€ Display extracted features in Build Details
```

---

## ğŸ“Š Entities & Relationships

### Main Entities

```
RawRepository
â”œâ”€ id
â”œâ”€ full_name (github: owner/repo)
â”œâ”€ github_repo_id
â”œâ”€ default_branch
â”œâ”€ is_private
â”œâ”€ main_lang
â””â”€ github_metadata

ModelRepoConfig
â”œâ”€ id
â”œâ”€ user_id
â”œâ”€ full_name
â”œâ”€ provider (github)
â”œâ”€ raw_repo_id â†’ RawRepository
â”œâ”€ installation_id
â”œâ”€ test_frameworks []
â”œâ”€ source_languages []
â”œâ”€ ci_provider
â”œâ”€ import_status (QUEUED â†’ IMPORTING â†’ IMPORTED â†’ FAILED)
â”œâ”€ max_builds_to_ingest
â”œâ”€ since_days
â”œâ”€ only_with_logs
â””â”€ last_sync_status

RawBuildRun
â”œâ”€ id
â”œâ”€ raw_repo_id â†’ RawRepository
â”œâ”€ build_id (CI provider build ID)
â”œâ”€ build_number
â”œâ”€ repo_name
â”œâ”€ branch
â”œâ”€ commit_sha
â”œâ”€ status (COMPLETED)
â”œâ”€ conclusion (SUCCESS/FAILURE/CANCELLED)
â”œâ”€ created_at
â”œâ”€ logs_available
â”œâ”€ logs_path
â”œâ”€ provider
â””â”€ raw_data

ModelTrainingBuild
â”œâ”€ id
â”œâ”€ raw_repo_id â†’ RawRepository
â”œâ”€ raw_workflow_run_id â†’ RawBuildRun
â”œâ”€ model_repo_config_id â†’ ModelRepoConfig
â”œâ”€ head_sha
â”œâ”€ build_number
â”œâ”€ build_created_at
â”œâ”€ build_conclusion
â”œâ”€ extraction_status (PENDING â†’ COMPLETED â†’ FAILED)
â”œâ”€ features {} (extracted features)
â”œâ”€ error_message
â””â”€ is_missing_commit
```

---

## ğŸ”„ Celery Task Chain

### Orchestration Chain

```
import_repo (ORCHESTRATOR)
  â”‚
  â””â”€â†’ chain(
      clone_repo.s(repo_id, full_name, installation_id),
      fetch_and_save_builds.s(repo_id, full_name, ..., ci_provider, ...),
      dispatch_processing.s(repo_id)
    )

dispatch_processing
  â”‚
  â””â”€â†’ group([
      process_workflow_run.s(repo_id, build_id_1),
      process_workflow_run.s(repo_id, build_id_2),
      ...
    ])
```

### Queue Configuration
- **import_repo queue**: clone_repo, fetch_and_save_builds, dispatch_processing
- **data_processing queue**: process_workflow_run

---

## ğŸŒ Real-time Updates (WebSocket)

### Events Published to Redis

**REPO_UPDATE**
```json
{
  "type": "REPO_UPDATE",
  "payload": {
    "repo_id": "...",
    "status": "importing | cloned | imported | failed",
    "message": "..."
  }
}
```

**BUILD_UPDATE**
```json
{
  "type": "BUILD_UPDATE",
  "payload": {
    "repo_id": "...",
    "build_id": "...",
    "status": "in_progress | completed | failed"
  }
}
```

---

## ğŸ“ API Endpoints (Synchronous Part)

| Method | Endpoint | Purpose |
|--------|----------|---------|
| GET | `/repos/search?q=query` | Search repos (returns private + public) |
| POST | `/repos/import/bulk` | Import multiple repositories |
| GET | `/repos/languages?full_name=owner/repo` | Detect repo languages |
| GET | `/repos/test-frameworks` | List supported test frameworks |
| GET | `/repos/` | List user's tracked repos |

---

## ğŸ”‘ Key Concepts

### Feature Extraction Pipeline (Hamilton DAG)

Features Ä‘Æ°á»£c extract tá»« 5 modules:
1. **build_features** - Test results, build time, failures
2. **git_features** - Commit history, branching patterns
3. **github_features** - PR reviews, issue stats
4. **repo_features** - Repository metadata
5. **log_features** - Build log analysis (test logs, compiler output)

### Swimlane Diagram Best Practice for Astah

1. **Swimlane width**: TÃ¹y proportional hoáº·c equal
2. **Activity shape**: Rounded rectangles
3. **Decision diamond**: For if/else conditions
4. **Merge/fork bar**: For parallel processing
5. **Timeline**: Top to bottom flow
6. **Cross-lane arrows**: Show interactions between lanes

### Warna Swimlane Recommendations

| Swimlane | Color | RGB |
|----------|-------|-----|
| UI / Frontend | Light Blue | #E1F5FE |
| API Layer | Light Purple | #F3E5F5 |
| Celery Tasks | Light Green | #E8F5E9 |
| Database | Light Orange | #FFF3E0 |
| External (GitHub/CI) | Light Pink | #FCE4EC |

---

## ğŸ’¡ Astah UML CÃ¡ch Táº¡o

### Step 1: Create Use Case Diagram
1. Open Astah â†’ New Project
2. Create Use Case Diagram
3. Add Actors:
   - Draw "Developer" actor
   - Draw "System" actor
4. Add Use Cases (UC1-UC9)
5. Draw Associations
6. Add Notes for descriptions

### Step 2: Create Activity Diagram
1. Create Activity Diagram
2. Add Swimlanes (5 cÃ¡i):
   - Right-click â†’ Insert Swimlane
   - Rename: UI / Frontend, API Layer, Celery Tasks, Database, External
3. Add Activities:
   - Double-click each swimlane area
   - Insert activities (rectangles)
   - Add decision points (diamonds)
4. Draw Flows:
   - Use arrows between activities
   - Cross-swimlane flows Ä‘á»ƒ show interactions

### Step 3: Add Details
1. Add notes/comments
2. Use guards `[condition]` on decision flows
3. Mark parallel flows with fork/join bars
4. Color-code activities if needed

---

## ğŸ“ Chi tiáº¿t API Flow

### 1. Search Repositories - `/repos/search`

```python
# Frontend
POST /api/repos/search?q="pytorch"

# Backend - RepositoryService.search_repositories()
User's GitHub Token â†’ GitHub API /search/repositories
                  â†’ /user/repos (private)
                  â†’ /user/installation/repositories (GitHub App)

# Response
{
  "private_matches": [
    {
      "id": 123,
      "name": "pytorch",
      "full_name": "pytorch/pytorch",
      "is_private": False,
      "description": "..."
    }
  ],
  "public_matches": [...]
}
```

### 2. Import Repositories - `/repos/import/bulk`

```python
# Frontend
POST /api/repos/import/bulk

Body:
[
  {
    "full_name": "pytorch/pytorch",
    "installation_id": "123456",
    "test_frameworks": ["pytest"],
    "source_languages": ["python"],
    "ci_provider": "github_actions",
    "max_builds": 100,
    "since_days": 180,
    "only_with_logs": true
  }
]

# Backend - RepositoryService.bulk_import_repositories()
for payload in payloads:
  1. Verify repo exists on GitHub
  2. Create/Update RawRepository
  3. Create/Update ModelRepoConfig
  4. celery: import_repo.delay()

Response:
[
  {
    "id": "...",
    "full_name": "pytorch/pytorch",
    "import_status": "QUEUED",
    "created_at": "..."
  }
]
```

---

## ğŸš€ Next Steps

1. Táº¡o Use Case Diagram trong Astah UML
2. Táº¡o Activity Diagram vá»›i 5 Swimlanes
3. Táº¡o Sequence Diagram cho chi tiáº¿t API flows
4. Táº¡o Class Diagram cho entities
5. Update thesis document

---

## ğŸ“Œ PlantUML Files Generated

âœ… `diagrams/flow1_use_case.puml` - Use Case Diagram
âœ… `diagrams/flow1_activity_swimlanes.puml` - Activity Diagram with Swimlanes

Báº¡n cÃ³ thá»ƒ:
- Má»Ÿ `.puml` files trong PlantUML editor Ä‘á»ƒ preview
- Export to SVG/PNG
- Import structure vÃ o Astah UML

