# Dataset Enrichment Pipeline - MÃ´ Táº£ Chi Tiáº¿t ToÃ n Bá»™ Luá»“ng

## ğŸ“‹ Má»¥c Lá»¥c
1. [Tá»•ng Quan Kiáº¿n TrÃºc](#tá»•ng-quan-kiáº¿n-trÃºc)
2. [Phase 1: Validation](#phase-1-validation)
3. [Phase 2: Ingestion](#phase-2-ingestion)
4. [Phase 3: Processing](#phase-3-processing)
5. [Scan Metrics Integration](#scan-metrics-integration)
6. [API Endpoints](#api-endpoints)
7. [Frontend UI Flow](#frontend-ui-flow)
8. [Entities & Data Model](#entities--data-model)
9. [Error Handling & Recovery](#error-handling--recovery)
10. [Performance Optimization](#performance-optimization)

---

## Tá»•ng Quan Kiáº¿n TrÃºc

### High-Level Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATASET ENRICHMENT PIPELINE                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User Upload CSV
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 1: VALIDATION (dataset_validation.py)
â”‚  âœ“ Validate repos trÃªn GitHub API
â”‚  âœ“ Validate builds trÃªn CI API
â”‚  âœ“ Apply build filters
â”‚  âœ“ Cache RawRepository & RawBuildRun
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 2: INGESTION (enrichment_ingestion.py)
â”‚  âœ“ Clone/update git repositories
â”‚  âœ“ Create git worktrees cho commits
â”‚  âœ“ Download build logs tá»« CI
â”‚  âœ“ Fork commit replay (náº¿u cáº§n)
â”‚  âœ“ Per-resource status tracking
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 3: PROCESSING (enrichment_processing.py)
â”‚  â”œâ”€ Async: Dispatch scans (Trivy, SonarQube)
â”‚  â””â”€ Sequential: Extract features (oldest â†’ newest)
â”‚      â”œâ”€ Hamilton DAG computation
â”‚      â”œâ”€ Backfill scan metrics
â”‚      â””â”€ Temporal features support
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
DatasetVersion (Enriched) + Scan Metrics
```

### Queue Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Celery Queue System   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ validation     â”‚ Validation tasks
â”‚ ingestion      â”‚ Clone, worktree, logs
â”‚ trivy_scan     â”‚ Trivy security scans
â”‚ sonar_scan     â”‚ SonarQube code quality
â”‚ processing     â”‚ Feature extraction, aggregation
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Phase 1: Validation

**File**: [backend/app/tasks/dataset_validation.py](backend/app/tasks/dataset_validation.py)

**Má»¥c Ä‘Ã­ch**: XÃ¡c minh táº¥t cáº£ repos vÃ  builds tá»« CSV thá»±c sá»± tá»“n táº¡i vÃ  cÃ³ thá»ƒ truy cáº­p

### 1.1 Tasks Overview

| Task | Queue | Timeout | Retries | MÃ´ Táº£ |
|------|-------|---------|---------|-------|
| `dataset_validation_orchestrator` | validation | 3660s | N/A | Äá»c CSV chunks, dispatch workers |
| `validate_repo_chunk` | validation | 660s | N/A | Validate batch repos qua GitHub API |
| `validate_builds_chunk` | validation | 360s | N/A | Validate batch builds qua CI API |
| `aggregate_validation_results` | validation | 360s | N/A | Tá»•ng há»£p results (chord callback) |

### 1.2 Validation Flow Diagram

```
dataset_validation_orchestrator
â”‚
â”œâ”€ Read CSV in chunks (SCAN_BUILDS_PER_QUERY=1000)
â”‚
â”œâ”€ For each chunk:
â”‚     â”œâ”€ validate_repo_chunk (parallel)
â”‚     â”‚   â”œâ”€ Check repo exists trÃªn GitHub
â”‚     â”‚   â”œâ”€ Cache RawRepository
â”‚     â”‚   â”œâ”€ Check repo is not private (unless authorized)
â”‚     â”‚   â””â”€ Create DatasetRepoStats (ci_provider, status)
â”‚     â”‚
â”‚     â””â”€ validate_builds_chunk (parallel per repo)
â”‚         â”œâ”€ Fetch build data tá»« CI provider (GitHub Actions, CircleCI, Travis)
â”‚         â”œâ”€ Apply build filters:
â”‚         â”‚   - status (success, failure, etc.)
â”‚         â”‚   - event_type (push, pull_request, etc.)
â”‚         â”‚   - branch_patterns (regex)
â”‚         â”œâ”€ Cache RawBuildRun
â”‚         â”œâ”€ Create DatasetBuild (if not exists)
â”‚         â””â”€ Mark as FOUND (if CI returned data)
â”‚
â””â”€ aggregate_validation_results (chord callback)
    â”œâ”€ Collect validation stats
    â”œâ”€ Update Dataset.validation_status â†’ READY
    â””â”€ Publish WebSocket event to frontend
```

### 1.3 Build Filters

CÃ¡c filter Ä‘Æ°á»£c apply trong validation phase:

```python
# From DatasetBuild entity
build_filters = {
    "status": ["success", "failure"],  # Only these statuses
    "event_types": ["push", "pull_request"],
    "branch_patterns": ["main", "develop", "release-*"]
}

# In validate_builds_chunk:
should_filter = should_filter_build(build_data)
if should_filter:
    build.status = "filtered"  # KhÃ´ng include trong enrichment
else:
    build.status = "found"
```

### 1.4 Data Structures Created

**DatasetBuild** (per CSV row):
```python
{
    dataset_id: ObjectId,
    build_id_from_csv: str,
    status: "found" | "not_found" | "filtered",
    raw_repo_id: ObjectId,      # Cache tá»« validation
    raw_run_id: ObjectId,        # Cache tá»« validation
    ci_provider: str,            # GitHub Actions, CircleCI, etc.
}
```

**RawRepository** (cache):
```python
{
    full_name: "owner/repo",
    github_repo_id: int,
    is_private: bool,
    ci_provider: str,
}
```

**RawBuildRun** (cache):
```python
{
    raw_repo_id: ObjectId,
    build_id_from_csv: str,
    ci_run_id: str,
    commit_sha: str,
    effective_sha: str,  # Updated if fork commit replay
    created_at: datetime,
    ci_provider: str,
}
```

**DatasetRepoStats**:
```python
{
    dataset_id: ObjectId,
    raw_repo_id: ObjectId,
    ci_provider: str,
    total_builds: int,
    found_builds: int,
    not_found_builds: int,
    filtered_builds: int,
}
```

### 1.5 Progress & Error Handling

- **Progress**: Publish tá»›i Redis + WebSocket cho frontend
- **Timeouts**: Soft time limit + error callback
- **Chunking**: MapReduce pattern vá»›i aggregation

---

## Phase 2: Ingestion

**File**: [backend/app/tasks/enrichment_ingestion.py](backend/app/tasks/enrichment_ingestion.py)

**Má»¥c Ä‘Ã­ch**: Thu tháº­p táº¥t cáº£ resources cáº§n thiáº¿t cho feature extraction

### 2.1 Concepts ChÃ­nh

#### 2.1.1 DatasetImportBuild Status Flow

```
                     â”Œâ”€ INGESTING â”€â”
                     â”‚             â”‚
PENDING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”œâ”€â”€â”€ INGESTED
                     â”‚             â”‚
                     â””â”€ ERROR â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                  MISSING_RESOURCE (graceful)
```

- **PENDING**: Chá» ingestion
- **INGESTING**: Äang clone, worktree, download logs
- **INGESTED**: Táº¥t cáº£ required resources Ä‘Ã£ sáºµn sÃ ng
- **MISSING_RESOURCE**: Má»™t sá»‘ resources bá»‹ lá»—i nhÆ°ng váº«n cÃ³ thá»ƒ process (graceful degradation)

#### 2.1.2 Resource DAG

```
clone_repo (git bare clone)
    â”‚
    â”œâ”€ create_worktrees (requires clone_repo)
    â”‚
    â””â”€ download_build_logs (parallel, independent)
```

**Task Dependencies**:
```python
TASK_DEPENDENCIES = {
    "clone_repo": [],
    "create_worktrees": ["clone_repo"],
    "download_build_logs": [],
}
```

#### 2.1.3 Dynamic Resource Calculation

Resources Ä‘Æ°á»£c tÃ­nh tá»« `selected_features`:

```python
# Tá»« feature_dag/_metadata.py
required_resources = get_required_resources_for_features(feature_set)

# VÃ­ dá»¥:
# Náº¿u selected_features = ["build_logs_duration", "git_diff_lines"]
# â†’ required_resources = {"build_logs", "git_worktree"}

# Náº¿u cÃ³ scan metrics â†’ FORCE git_worktree (needed for scans)
if has_sonarqube or has_trivy:
    required_resources.add("git_worktree")
```

### 2.2 Tasks Overview

| Task | Queue | Timeout | Retries | MÃ´ Táº£ |
|------|-------|---------|---------|-------|
| `start_enrichment` | processing | 180s | N/A | Orchestrator: build chains, dispatch chord |
| `clone_repo` | ingestion | 660s | 3 | Clone/update bare git repo |
| `create_worktree_chunk` | ingestion | 660s | 2 | Create worktrees cho commits (sequential) |
| `download_logs_chunk` | ingestion | 300s | 2 | Download logs tá»« CI (parallel chunks) |
| `aggregate_ingestion_results` | processing | 60s | N/A | Aggregate results (chord callback) |
| `aggregate_logs_results` | ingestion | 120s | N/A | Aggregate log chunk results (chord callback) |
| `reingest_failed_builds` | processing | 360s | N/A | Retry FAILED builds (not MISSING_RESOURCE) |

### 2.3 Ingestion Workflow Diagram

```
start_enrichment
â”‚
â”œâ”€ Validate version exists
â”œâ”€ Create DatasetImportBuild records (one per validated build)
â”œâ”€ Get required resources from feature selection
â”‚
â””â”€ Build & dispatch CHORD:
    â”‚
    â”œâ”€ GROUP of CHAINS (parallel repos):
    â”‚   â”‚
    â”‚   â”œâ”€ Repo 1: chain(clone_repo â†’ create_worktrees â†’ download_logs)
    â”‚   â”œâ”€ Repo 2: chain(clone_repo â†’ create_worktrees â†’ download_logs)
    â”‚   â””â”€ Repo N: chain(clone_repo â†’ create_worktrees â†’ download_logs)
    â”‚
    â””â”€ CALLBACK: aggregate_ingestion_results
        â”‚
        â”œâ”€ Parse results from all repos
        â”œâ”€ Update per-resource status in DatasetImportBuild
        â”œâ”€ Mark builds as INGESTED or MISSING_RESOURCE
        â”œâ”€ Publish WebSocket event
        â””â”€ User can now start Phase 3 processing
```

### 2.4 Per-Resource Status Tracking

```python
# In DatasetImportBuild entity
resource_status = {
    "git_history": {
        "status": "completed|failed|pending|skipped",
        "error": "optional error message",
        "started_at": datetime,
        "completed_at": datetime,
    },
    "git_worktree": {
        "status": "completed",
        "error": None,
        ...
    },
    "build_logs": {
        "status": "failed",
        "error": "Log expired on CI provider",
        ...
    }
}
```

**Status Update Logic**:
1. Clone fails â†’ ALL builds: git_history = FAILED
2. Worktree fails for commit X â†’ Builds with commit X: git_worktree = FAILED
3. Log download fails for build Y â†’ Build Y: build_logs = FAILED

### 2.5 Clone Repository Task

**File**: [backend/app/tasks/shared/ingestion_tasks.py](backend/app/tasks/shared/ingestion_tasks.py)

```python
clone_repo(
    raw_repo_id: str,
    github_repo_id: int,
    full_name: str,
    correlation_id: str,
)
```

**Flow**:
1. Acquire Redis lock (repo-level) - prevent concurrent clones
2. Check if repo already cloned â†’ use git fetch --all --prune
3. If not cloned â†’ git clone --bare (bare repo for efficiency)
4. For org repos (configured GITHUB_INSTALLATION_ID):
   - Get installation token tá»« GitHub App
   - Use token authentication (higher rate limits)
5. Timeout 600s, max retries 3

**Result**:
```python
{
    "resource": "git_history",
    "status": "cloned|failed|timeout",
    "path": "/data/repos/<github_repo_id>.git",
    "error": "optional error message",
}
```

### 2.6 Worktree Creation Task

**File**: [backend/app/tasks/shared/ingestion_tasks.py](backend/app/tasks/shared/ingestion_tasks.py)

```python
create_worktree_chunk(
    raw_repo_id: str,
    github_repo_id: int,
    commit_shas: List[str],  # Chunk of commits
    chunk_index: int,
    total_chunks: int,
    correlation_id: str,
)
```

**Flow**:
1. For each commit in chunk:
   - Check if worktree already exists â†’ skip
   - Check if commit exists locally: `git cat-file -e <sha>`
   - If not exists locally â†’ Fork commit replay (special handling)
   - Create worktree: `git worktree add --detach <path> <sha>`
2. Return with created/skipped/failed counts
3. Timeout 600s, max retries 2 (due to fork replay complexity)

#### Fork Commit Replay
Xá»­ lÃ½ commits tá»« fork PRs mÃ  khÃ´ng cÃ³ trong base repo:

```
Scenario: User creates PR tá»« fork, commit SHA khÃ´ng trong base repo

1. _commit_exists_locally(repo_path, sha) â†’ False
2. ensure_commit_exists(repo_path, commit_sha, repo_slug, github_client)
   â”œâ”€ Fetch patch tá»« GitHub API
   â”œâ”€ Apply patch lÃªn base repo
   â”œâ”€ Create synthetic commit
   â””â”€ Return synthetic_sha (cÃ³ thá»ƒ khÃ¡c original)
3. Update RawBuildRun.effective_sha = synthetic_sha
4. Create worktree sá»­ dá»¥ng synthetic_sha
```

**Result**:
```python
{
    "resource": "git_worktree",
    "chunk_index": 0,
    "worktrees_created": 150,
    "worktrees_skipped": 50,
    "worktrees_failed": 5,
    "fork_commits_replayed": 3,
    "failed_commits": ["abc123", ...],
    "created_commits": ["def456", ...],
}
```

### 2.7 Log Download Task

**File**: [backend/app/tasks/shared/ingestion_tasks.py](backend/app/tasks/shared/ingestion_tasks.py)

```python
download_logs_chunk(
    raw_repo_id: str,
    github_repo_id: int,
    full_name: str,
    build_ids: List[str],  # Chunk of builds
    ci_provider: str,
    chunk_index: int,
    total_chunks: int,
    correlation_id: str,
)
```

**Flow**:
1. Fetch build details tá»« CI provider (async/concurrent)
2. For each build:
   - Check if logs already exist locally
   - Download logs tá»« CI API
   - Parse logs (extract durations, errors, stages)
   - Save to: `/data/repos/<github_repo_id>/logs/<ci_run_id>/`
3. Return with downloaded/skipped/failed counts
4. Timeout 300s, max retries 2

**Log Aggregation** (chord callback):
```python
aggregate_logs_results(
    chunk_results: List[Dict],
)
```

Tá»•ng há»£p results tá»« táº¥t cáº£ parallel chunks.

**Result**:
```python
{
    "resource": "build_logs",
    "chunk_index": 0,
    "logs_downloaded": 500,
    "logs_skipped": 100,
    "logs_failed": 20,
    "expired_log_ids": ["build_5", ...],  # Expired on CI
    "failed_log_ids": ["build_6", ...],
}
```

### 2.8 Aggregation Logic

**aggregate_ingestion_results** (chord callback):

```python
def aggregate_ingestion_results(
    results: List[Dict],
    version_id: str,
    correlation_id: str,
):
    # Parse results tá»« táº¥t cáº£ ingestion chains
    clone_failed = check_if_clone_failed(results)
    failed_commits = collect_failed_commits(results)
    failed_log_ids = collect_failed_logs(results)
    
    # Update per-resource status cho táº¥t cáº£ builds
    if clone_failed:
        # Táº¥t cáº£ builds: git_history = FAILED
        update_resource_status_batch(
            version_id,
            "git_history",
            ResourceStatus.FAILED,
            error_msg
        )
    else:
        # Táº¥t cáº£ builds: git_history = COMPLETED
        update_resource_status_batch(
            version_id,
            "git_history",
            ResourceStatus.COMPLETED
        )
    
    # Worktree failures
    if failed_commits:
        # Only builds with these commits: git_worktree = FAILED
        update_resource_by_commits(
            version_id,
            "git_worktree",
            failed_commits,
            ResourceStatus.FAILED
        )
    
    # Log failures
    if failed_log_ids:
        # Only these builds: build_logs = FAILED
        update_resource_by_build_ids(
            version_id,
            "build_logs",
            failed_log_ids,
            ResourceStatus.FAILED
        )
    
    # Mark final status
    if clone_failed:
        # All builds cannot proceed
        mark_all_as(version_id, MISSING_RESOURCE)
    else:
        # Mark by resource status
        if has_failed_resources:
            mark_as_missing_resource(version_id)
        else:
            mark_as_ingested(version_id)
    
    # Publish event
    publish_enrichment_update(version_id, "ingested")
```

### 2.9 Graceful Degradation

**Design Philosophy**: "Báº¥t ká»³ build nÃ o cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c processed, ngay cáº£ thiáº¿u resources"

- Build cÃ³ táº¥t cáº£ resources â†’ Extract táº¥t cáº£ features
- Build missing git_worktree â†’ KhÃ´ng thá»ƒ extract git_diff features, nhÆ°ng váº«n cÃ³ log features
- Build missing build_logs â†’ KhÃ´ng thá»ƒ extract log features, nhÆ°ng váº«n cÃ³ git features
- Build missing git_history â†’ Chá»‰ cÃ³ metadata features

**Matching Model Pipeline**: CÃ¹ng design pattern vá»›i model_pipeline

### 2.10 Retry Failed Builds

**reingest_failed_builds** task:

```python
def reingest_failed_builds(version_id: str):
    # Find FAILED builds (retryable - actual errors like timeout, network)
    # Does NOT retry MISSING_RESOURCE (not retryable - logs expired)
    failed = find_by_status(version_id, FAILED)
    
    # Reset them to PENDING, clear error fields
    for build in failed:
        update_status(build.id, PENDING)
        clear_error_fields(build.id)
    
    # Re-trigger ingestion
    start_enrichment.delay(version_id)
```

Cho phÃ©p user thá»­ láº¡i ingestion cho builds cÃ³ transient errors (timeout, network).

---

## Phase 3: Processing

**File**: [backend/app/tasks/enrichment_processing.py](backend/app/tasks/enrichment_processing.py)

**Má»¥c Ä‘Ã­ch**: Extract features tá»« ingested resources + backfill scan metrics

### 3.1 Overview

```
User manually triggers start_enrichment_processing
    â”‚
    â”œâ”€ Validate version status = INGESTED
    â”‚
    â””â”€ dispatch_scans_and_processing
        â”œâ”€ dispatch_version_scans (async, fire & forget)
        â”‚   â””â”€ Scan only unique commits (once per version)
        â”‚
        â””â”€ dispatch_enrichment_batches (sequential)
            â””â”€ chain(B1 â†’ B2 â†’ ... â†’ Bn â†’ finalize)
```

### 3.2 Why Sequential Processing?

**Temporal Features**: Má»™t sá»‘ features phá»¥ thuá»™c vÃ o builds trÆ°á»›c Ä‘Ã³:

```
build_history_failure_rate = (failures_in_last_N_builds / N)
author_commit_count = tá»•ng commits tá»« author tÃ­nh Ä‘áº¿n build nÃ y
build_duration_trend = xu hÆ°á»›ng thá»i gian build qua cÃ¡c builds
```

Náº¿u build N fail â†’ táº¥t cáº£ builds sau Ä‘Ã³ sáº½ cÃ³ temporal features khÃ´ng chÃ­nh xÃ¡c!

**Solution**: Process tuáº§n tá»± tá»« cÅ© â†’ má»›i:
```
1. B1 (oldest) â†’ complete â†’ update DB
2. B2 â†’ use B1's results â†’ complete â†’ update DB
3. B3 â†’ use B1, B2's results â†’ complete
...
n. Bn (newest) â†’ use all previous â†’ complete
n+1. finalize_enrichment â†’ aggregate from DB
```

### 3.3 Tasks Overview

| Task | Queue | Timeout | MÃ´ Táº£ |
|------|-------|---------|-------|
| `start_enrichment_processing` | processing | 120s | User entry point, validate status |
| `dispatch_scans_and_processing` | processing | 60s | Dispatch scans async + processing sequential |
| `dispatch_version_scans` | processing | 600s | Paginate builds, collect unique commits, dispatch scans |
| `dispatch_scan_for_commit` | processing | 120s | Dispatch Trivy + SonarQube cho 1 commit |
| `dispatch_enrichment_batches` | processing | 180s | Create enrichment builds, dispatch chain |
| `process_single_enrichment` | processing | 600s | Extract features cho 1 build (sequential) |
| `finalize_enrichment` | processing | 60s | Aggregate results, mark completed |
| `reprocess_failed_enrichment_builds` | processing | 360s | Retry failed builds |
| `process_version_export_job` | processing | 900s | Export version thÃ nh CSV/JSON |
| `start_trivy_scan_for_version_commit` | trivy_scan | 900s | Run Trivy CLI scan |
| `start_sonar_scan_for_version_commit` | sonar_scan | 2100s | Submit SonarQube scan |
| `export_metrics_from_webhook` | processing | 180s | Handle SonarQube webhook callback |

### 3.4 Feature Extraction Pipeline

**extract_features_for_build** (shared helper):

```python
result = extract_features_for_build(
    db=db,
    raw_repo=raw_repo,
    feature_config=dataset_version.feature_configs,
    raw_build_run=raw_build_run,
    selected_features=selected_features,
    output_build_id=enrichment_build_id,
    category=AuditLogCategory.DATASET_ENRICHMENT,
)

# Returns:
{
    "status": "completed|partial|failed",
    "feature_vector_id": ObjectId,
    "feature_count": int,
    "errors": List[str],
}
```

**Hamilton DAG Execution**:

```python
# From hamilton_runner.py
pipeline = HamiltonPipeline(...)

# Inputs (prepared by input_preparer.py):
inputs = {
    "repo": raw_repo,
    "build_run": raw_build_run,
    "feature_config": feature_config,
    "git_history": git_history_obj,        # if available
    "git_worktree": git_worktree_obj,      # if available
    "build_logs": build_logs_obj,          # if available
    "raw_build_runs": collection,          # for temporal features
    "feature_vectors": collection,         # for temporal features
}

# Execute
features = pipeline.execute(
    selected_features=selected_features,
    inputs=inputs,
    config=feature_config,
)

# Store in FeatureVector (single source of truth)
feature_vector = FeatureVectorRepository.upsert(
    build_id=raw_build_run.id,
    version_id=version_id,
    features=features,
    category=AuditLogCategory.DATASET_ENRICHMENT,
)
```

**Feature Categories** (from _metadata.py):
- BUILD_LOG: Duration, status, stages
- GIT_HISTORY: Commits, authors, deletions
- GIT_DIFF: Lines changed, complexity
- REPO_SNAPSHOT: Language, stars, watchers
- PR_INFO: Title, description, comments
- DISCUSSION: Comments, reviews
- TEAM: Authors, reviewers
- METADATA: Timestamp, build number
- WORKFLOW: Triggers, matrix builds
- DEVOPS: DevOps files (GitHub Actions, etc.)
- BUILD_HISTORY: Previous build features
- COMMITTER: Commit author experience
- COOPERATION: Distinct authors, revisions

### 3.5 Scan Metrics Integration

#### 3.5.1 Trivy Vulnerability Scanning

**File**: [backend/app/tasks/trivy.py](backend/app/tasks/trivy.py)

```
dispatch_scan_for_commit
    â””â”€ dispatch_scan_for_commit.delay(
        version_id, raw_repo_id, github_repo_id,
        commit_sha, repo_full_name
    )
        â””â”€ start_trivy_scan_for_version_commit
            â”œâ”€ Create/get TrivyCommitScan record
            â”œâ”€ Get worktree path tá»« github_repo_id + commit_sha
            â”œâ”€ Run TrivyTool.scan(target_path, scan_types, config_file)
            â”‚   â””â”€ Trivy CLI: --format json --server mode hoáº·c standalone
            â”œâ”€ Parse metrics (vuln_total, critical, high, medium, low)
            â”œâ”€ Filter metrics based on selected_metrics config
            â”œâ”€ Backfill to FeatureVector.scan_metrics:
            â”‚   {
            â”‚       "trivy_vuln_total": 42,
            â”‚       "trivy_vuln_critical": 3,
            â”‚       "trivy_vuln_high": 8,
            â”‚       "trivy_scan_duration_ms": 1250,
            â”‚   }
            â”‚   (via DatasetEnrichmentBuild â†’ FeatureVector reference)
            â”œâ”€ Mark TrivyCommitScan as COMPLETED
            â””â”€ Return results
```

**Trivy Config**:
```python
trivy_config = {
    "scanners": ["vuln", "config", "secret"],  # default
    "trivyYaml": "...",  # Custom config content
    "extraArgs": "--severity HIGH,CRITICAL",
}

# Per-repo override:
scan_config = {
    "repos": {
        "12345": {  # github_repo_id
            "scanners": ["vuln"],
            "trivyYaml": "...",
        }
    },
    # default for other repos
    "scanners": ["vuln", "config"],
}
```

#### 3.5.2 SonarQube Code Quality Analysis

**File**: [backend/app/tasks/sonar.py](backend/app/tasks/sonar.py)

```
dispatch_scan_for_commit
    â””â”€ start_sonar_scan_for_version_commit
        â”œâ”€ Create/get SonarCommitScan record
        â”œâ”€ Generate component_key = "<version>_<repo>_<commit>"
        â”œâ”€ Run sonar-scanner CLI:
        â”‚   sonar-scanner \
        â”‚     -Dsonar.projectKey=<component_key> \
        â”‚     -Dsonar.sources=. \
        â”‚     ...
        â”œâ”€ Mark SonarCommitScan as SCANNING
        â””â”€ Return (don't wait for results)

Then (async, via webhook):
        â”œâ”€ SonarQube completes analysis...
        â”‚
        â””â”€ SonarQube â†’ POST /api/webhook/sonarqube
                       â”œâ”€ export_metrics_from_webhook
                       â”‚   â”œâ”€ Find SonarCommitScan by component_key
                       â”‚   â”œâ”€ Fetch metrics tá»« SonarQube API
                       â”‚   â”‚   (blocks, duplicates, complexity, coverage)
                       â”‚   â”œâ”€ Filter by selected_metrics
                       â”‚   â”œâ”€ Backfill to FeatureVector.scan_metrics:
                       â”‚   â”‚   {
                       â”‚   â”‚       "sonar_bugs": 5,
                       â”‚   â”‚       "sonar_code_smells": 12,
                       â”‚   â”‚       "sonar_coverage": 75.5,
                       â”‚   â”‚       "sonar_complexity": 42,
                       â”‚   â”‚   }
                       â”‚   â”‚   (via DatasetEnrichmentBuild â†’ FeatureVector reference)
                       â”‚   â”œâ”€ Mark SonarCommitScan as COMPLETED
                       â”‚   â””â”€ Return results
```

**SonarQube Config**:
```python
sonar_config = {
    "projectKey": "my_project",
    "extraProperties": "sonar.java.coveragePlugin=jacoco",
}

# Per-repo override:
scan_config = {
    "repos": {
        "12345": {
            "projectKey": "my_repo_12345",
            "extraProperties": "...",
        }
    },
    # default
    "projectKey": "default_key",
}
```

#### 3.5.3 Backfill Pattern

Scan results Ä‘Æ°á»£c **backfill** tá»›i táº¥t cáº£ builds cÃ³ cÃ¹ng commit:

```python
# In TrivyTool hoáº·c SonarQube callback
updated_count = enrichment_build_repo.backfill_by_commit_in_version(
    version_id=ObjectId(version_id),
    commit_sha=commit_sha,
    scan_features={
        "vuln_total": 42,
        "vuln_critical": 3,
        ...
    },
    prefix="trivy_",  # Táº¡o keys nhÆ° "trivy_vuln_total"
)

# Flow:
# 1. Find all DatasetEnrichmentBuilds with this commit in version
# 2. For each build, get feature_vector_id
# 3. Update FeatureVector.scan_metrics["trivy_vuln_total"] = 42
# 
# Result: All FeatureVectors for builds with this commit get scan metrics
```

#### 3.5.4 Scan Dispatch Strategy

**dispatch_version_scans** (per-commit, not per-build):

```
1. Paginate through all builds in version (SCAN_BUILDS_PER_QUERY=1000)
2. For each page:
   â”œâ”€ Batch query RawBuildRuns (fetch commit SHAs)
   â”œâ”€ Collect unique (repo_id, commit_sha) pairs
   â”œâ”€ If reach SCAN_COMMITS_PER_BATCH=100:
   â”‚   â””â”€ dispatch_scan_batch(batch_100_commits)
   â”‚       â””â”€ group(dispatch_scan_for_commit, ...)
   â”‚
3. Dispatch remaining commits
4. Return stats
```

**Why per-commit**: TrÃ¡nh duplicate scans náº¿u nhiá»u builds cÃ³ cÃ¹ng commit

**Why async**: Scans cÃ³ thá»ƒ cháº¡y lÃ¢u (10-30 phÃºt cho SonarQube), khÃ´ng block feature extraction

### 3.6 Feature Extraction Task

**process_single_enrichment**:

```python
def process_single_enrichment(
    version_id: str,
    enrichment_build_id: str,
    selected_features: List[str],
    correlation_id: str,
):
    # Get entities
    enrichment_build = find_by_id(enrichment_build_id)
    raw_build_run = find_by_id(enrichment_build.raw_build_run_id)
    raw_repo = find_by_id(raw_build_run.raw_repo_id)
    
    # Skip if already processed
    if enrichment_build.extraction_status != PENDING:
        return {"status": "skipped"}
    
    # Extract features
    result = extract_features_for_build(
        db=db,
        raw_repo=raw_repo,
        feature_config=dataset_version.feature_configs,
        raw_build_run=raw_build_run,
        selected_features=selected_features,
        output_build_id=enrichment_build_id,
        category=AuditLogCategory.DATASET_ENRICHMENT,
    )
    
    # Update enrichment build
    enrichment_build_repo.update_one(enrichment_build_id, {
        "feature_vector_id": result["feature_vector_id"],
        "extraction_status": result["status"],  # completed|partial|failed
        "extraction_error": result.get("errors"),
        "enriched_at": datetime.now(),
    })
    
    # Update version progress
    version_repo.increment_builds_features_extracted(version_id)
    
    return {"status": result["status"], "feature_count": result.get("feature_count")}
```

### 3.7 Finalization Task

**finalize_enrichment** (end of sequential chain):

```python
def finalize_enrichment(
    version_id: str,
    created_count: int,
    correlation_id: str,
):
    # Get aggregated stats from DB (no Redis tracker)
    stats = enrichment_build_repo.aggregate_stats_by_version(version_id)
    # {completed: X, partial: Y, failed: Z}
    
    # Determine final status
    if failed > 0 and completed == 0:
        final_status = FAILED
    else:
        final_status = PROCESSED
    
    # Update version with feature_extraction_completed flag
    version_repo.update_one(version_id, {
        "status": final_status,
        "builds_features_extracted": completed + partial,
        "builds_extraction_failed": failed,
        "feature_extraction_completed": True,  # Mark features done
    })
    
    # Auto quality evaluation
    if final_status == PROCESSED:
        quality_service.evaluate_version(version_id)
    
    # Check if enrichment fully complete (features + scans)
    check_and_notify_enrichment_completed(version_id)
    
    # Publish event
    publish_enrichment_update(version_id, final_status, feature_extraction_completed=True)
```

### 3.8 Error Handling

**handle_enrichment_processing_chain_error** (error callback):

```
Chain fails (worker crash, timeout, unhandled exception)
    â”œâ”€ Mark all IN_PROGRESS builds as FAILED
    â”œâ”€ Count completed vs failed
    â”œâ”€ Update version status (PROCESSED if some completed, else FAILED)
    â””â”€ Publish event to frontend
```

**Temporal Feature Integrity**: Náº¿u build N fail, táº¥t cáº£ builds sau sáº½ khÃ´ng cÃ³ correct temporal features. Design yÃªu cáº§u user review vÃ  reprocess náº¿u cáº§n.

### 3.9 Retry Failed Builds

**reprocess_failed_enrichment_builds**:

```python
def reprocess_failed_enrichment_builds(version_id: str):
    # Find FAILED enrichment builds
    failed = find_by_status(version_id, FAILED)
    
    # Reset to PENDING
    for build in failed:
        update_status(build.id, PENDING)
    
    # Dispatch sequential chain again
    # (same as normal processing, but for failed builds only)
    
    # Dispatch chain: B_failed1 â†’ B_failed2 â†’ ... â†’ finalize
```

### 3.10 Export Job

**process_version_export_job**:

```python
def process_version_export_job(job_id: str):
    job = find_by_id(job_id)
    
    # Validate
    assert job.dataset_id and job.version_id
    
    # Get enriched builds cursor (with FeatureVector join)
    cursor = enrichment_build_repo.get_enriched_for_export(
        dataset_id=job.dataset_id,
        version_id=job.version_id,
    )
    # Returns: DatasetEnrichmentBuild joined with FeatureVector
    # Each row contains: build metadata + features + scan_metrics
    
    # Get all feature keys (for CSV headers)
    all_feature_keys = enrichment_build_repo.get_all_feature_keys(...)
    # Includes: FeatureVector.features keys + FeatureVector.scan_metrics keys
    
    # Write CSV or JSON
    if job.format == "csv":
        write_csv_file(
            file_path=EXPORTS_DIR / filename,
            cursor=cursor,
            features=job.features,
            all_feature_keys=all_feature_keys,
            progress_callback=update_progress,
        )
    
    # Mark completed
    job_repo.update_status(job_id, "completed", file_path=file_path)
```

---

## Scan Metrics Integration

### Scan Tools Configuration

#### Trivy Tool

**File**: [backend/app/integrations/tools/trivy/tool.py](backend/app/integrations/tools/trivy/tool.py)

```python
class TrivyTool:
    def scan(
        self,
        target_path: str,
        scan_types: List[str] = None,  # ["vuln", "config", "secret"]
        config_file_path: Path = None,
    ) -> Dict[str, Any]:
        """
        Run Trivy scan on target directory/image.
        
        Returns:
        {
            "status": "success|failed",
            "metrics": {
                "vuln_total": int,
                "vuln_critical": int,
                "vuln_high": int,
                "vuln_medium": int,
                "vuln_low": int,
                "config_issues": int,
                "secret_issues": int,
            },
            "scan_duration_ms": int,
        }
        """
```

**Modes**:
- **Server mode** (recommended): Uses Trivy server via `--server` flag
- **Standalone**: Runs Trivy Docker image directly

**Configuration**:
- Load tá»« DB settings (SettingsService)
- Fallback to ENV vars (TRIVY_SERVER_URL)
- Custom config file per repo/version

#### SonarQube Tool

**File**: [backend/app/integrations/tools/sonarqube/tool.py](backend/app/integrations/tools/sonarqube/tool.py)

```python
class SonarQubeTool:
    def scan_commit(
        self,
        commit_sha: str,
        full_name: str,
        config_file_path: Path = None,
        shared_worktree_path: str = None,
    ) -> None:
        """
        Initiate SonarQube scan using sonar-scanner CLI.
        
        Results delivered via webhook callback (async).
        Does not wait for completion.
        """
```

**Modes**:
- **Async via webhook**: Submit scan, wait for webhook callback
- Token authentication (from DB settings or ENV)

**Configuration**:
- Load tá»« DB settings (SettingsService)
- Fallback to ENV vars (SONAR_HOST_URL, SONAR_TOKEN)
- Custom config per repo/version

#### Metrics Exporter

**File**: [backend/app/integrations/tools/sonarqube/exporter.py](backend/app/integrations/tools/sonarqube/exporter.py)

```python
class MetricsExporter:
    def collect_metrics(
        self,
        component_key: str,
        selected_metrics: List[str] = None,
    ) -> Dict[str, Any]:
        """
        Fetch metrics from SonarQube API.
        
        Returns only selected metrics if specified.
        """
```

### Scan Metrics Selection

**In DatasetVersion**:
```python
scan_metrics = {
    "trivy": ["vuln_total", "vuln_critical"],
    "sonarqube": ["bugs", "code_smells", "coverage"],
}

scan_config = {
    "trivy": {
        "scanners": ["vuln"],
        "trivyYaml": "...",
    },
    "sonarqube": {
        "projectKey": "...",
        "extraProperties": "...",
    },
}
```

### Metrics Filtering

```python
# In start_trivy_scan_for_version_commit
filtered_metrics = _filter_trivy_metrics(
    raw_metrics={
        "vuln_total": 42,
        "vuln_critical": 3,
        "config_issues": 5,
        "secret_issues": 0,
    },
    selected_metrics=["vuln_total", "vuln_critical"],
)
# Result: {"vuln_total": 42, "vuln_critical": 3}
```

---

## API Endpoints

**File**: [backend/app/api/dataset_versions.py](backend/app/api/dataset_versions.py)

> [!NOTE]
> Prefix: `/datasets/{dataset_id}/versions`

### Version Management

| Method | Endpoint | MÃ´ Táº£ |
|--------|----------|-------|
| `GET` | `/` | List versions for a dataset |
| `POST` | `/` | Create new version (triggers validation & ingestion) |
| `GET` | `/{version_id}` | Get version details |
| `DELETE` | `/{version_id}` | Delete version (cascade deletes builds) |

### Ingestion & Processing

| Method | Endpoint | MÃ´ Táº£ |
|--------|----------|-------|
| `GET` | `/{version_id}/import-builds` | List DatasetImportBuild records |
| `GET` | `/{version_id}/enrichment-builds` | List DatasetEnrichmentBuild records |
| `GET` | `/{version_id}/builds/{build_id}` | Get build detail with features |
| `POST` | `/{version_id}/start-processing` | Start processing phase (requires INGESTED status) |
| `POST` | `/{version_id}/retry-ingestion` | Retry FAILED ingestion builds |
| `POST` | `/{version_id}/retry-processing` | Retry FAILED processing builds |

### Scan Metrics

| Method | Endpoint | MÃ´ Táº£ |
|--------|----------|-------|
| `GET` | `/{version_id}/scan-status` | Get scan status summary (Trivy/SonarQube) |
| `GET` | `/{version_id}/commit-scans` | List commit scans with pagination |
| `GET` | `/{version_id}/commit-scans/{commit_sha}` | Get scan detail for specific commit |
| `POST` | `/{version_id}/commit-scans/{commit_sha}/retry` | Retry scan for commit (tool_type param) |

### Data & Export

| Method | Endpoint | MÃ´ Táº£ |
|--------|----------|-------|
| `GET` | `/{version_id}/data` | Get paginated data with column stats |
| `GET` | `/{version_id}/preview` | Preview exportable data |
| `GET` | `/{version_id}/export` | Stream export (CSV/JSON) - small datasets |
| `POST` | `/{version_id}/export/async` | Create async export job - large datasets |
| `GET` | `/{version_id}/export/jobs` | List export jobs |
| `GET` | `/export/jobs/{job_id}` | Get export job status |
| `GET` | `/export/jobs/{job_id}/download` | Download completed export file |

### Quality Evaluation

| Method | Endpoint | MÃ´ Táº£ |
|--------|----------|-------|
| `POST` | `/{version_id}/evaluate` | Start quality evaluation |
| `GET` | `/{version_id}/quality-report` | Get latest quality report |

---

## Frontend UI Flow

**Files**: [frontend/src/app/(app)/projects/](frontend/src/app/(app)/projects/)

### Page Structure

```
/projects
â”œâ”€â”€ page.tsx                    # Dataset list page
â”œâ”€â”€ layout.tsx                  # Main layout
â”œâ”€â”€ _components/
â”‚   â””â”€â”€ StatusBadge.tsx         # Dataset status badge component
â”œâ”€â”€ upload/
â”‚   â”œâ”€â”€ page.tsx                # CSV upload wizard
â”‚   â””â”€â”€ _components/
â”‚       â””â”€â”€ ...                 # Upload-related components
â””â”€â”€ [datasetId]/
    â”œâ”€â”€ page.tsx                # Dataset detail (version history)
    â”œâ”€â”€ layout.tsx              # Dataset layout with tabs
    â”œâ”€â”€ _components/
    â”‚   â”œâ”€â”€ CorrelationMatrixModal.tsx
    â”‚   â”œâ”€â”€ DatasetHeader.tsx
    â”‚   â”œâ”€â”€ FeatureDistributionModal.tsx
    â”‚   â”œâ”€â”€ VersionHistory.tsx
    â”‚   â”œâ”€â”€ VersionHistoryTable.tsx
    â”‚   â”œâ”€â”€ FeatureSelection/
    â”‚   â”‚   â””â”€â”€ ...             # Feature selection components
    â”‚   â””â”€â”€ tabs/
    â”‚       â””â”€â”€ ...             # Tab components
    â”œâ”€â”€ builds/
    â”‚   â””â”€â”€ page.tsx            # Builds by dataset
    â””â”€â”€ versions/
        â”œâ”€â”€ new/
        â”‚   â””â”€â”€ page.tsx        # Create new version wizard
        â””â”€â”€ [versionId]/
            â”œâ”€â”€ layout.tsx      # Version layout with tabs
            â”œâ”€â”€ page.tsx        # Version dashboard
            â”œâ”€â”€ _components/
            â”‚   â”œâ”€â”€ VersionDashboard.tsx
            â”‚   â”œâ”€â”€ VersionMiniStepper.tsx    # 2-phase stepper (Ingestion â†’ Processing)
            â”‚   â”œâ”€â”€ VersionIngestionCard.tsx
            â”‚   â”œâ”€â”€ VersionProcessingCard.tsx
            â”‚   â”œâ”€â”€ AnalysisSection.tsx
            â”‚   â”œâ”€â”€ ExportSection.tsx
            â”‚   â”œâ”€â”€ PreprocessingSection.tsx
            â”‚   â”œâ”€â”€ ScanMetricsSection.tsx
            â”‚   â”œâ”€â”€ FeatureDistributionChart.tsx
            â”‚   â”œâ”€â”€ FeatureDistributionCarousel.tsx
            â”‚   â””â”€â”€ CorrelationMatrixChart.tsx
            â”œâ”€â”€ _hooks/
            â”‚   â””â”€â”€ ...                       # Version-related hooks
            â”œâ”€â”€ builds/
            â”‚   â”œâ”€â”€ layout.tsx
            â”‚   â”œâ”€â”€ page.tsx
            â”‚   â”œâ”€â”€ ingestion/
            â”‚   â”‚   â””â”€â”€ page.tsx              # Ingestion builds table
            â”‚   â”œâ”€â”€ processing/
            â”‚   â”‚   â””â”€â”€ page.tsx              # Processing builds table
            â”‚   â””â”€â”€ scans/
            â”‚       â””â”€â”€ ...                   # Scan results pages
            â”œâ”€â”€ analysis/
            â”‚   â””â”€â”€ page.tsx                  # Feature analysis page
            â””â”€â”€ export/
                â””â”€â”€ page.tsx                  # Export configuration page
```

### Upload Flow UI

```
Step 1: Upload CSV
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚           â”‚    ğŸ“„ Drag & Drop CSV file        â”‚             â”‚
â”‚           â”‚    or click to browse             â”‚             â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                              â”‚
â”‚    CSV Format: repo_name, build_id, ...                     â”‚
â”‚                                                              â”‚
â”‚                                               [Next â†’]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 2: Configure Dataset
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Dataset Name: [___________________]                       â”‚
â”‚                                                              â”‚
â”‚    Description: [___________________]                        â”‚
â”‚                                                              â”‚
â”‚    Template: [Risk Prediction â–¼]                            â”‚
â”‚                                                              â”‚
â”‚                                       [â† Back] [Create]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Version Dashboard Components

```
### Version Dashboard Page (`page.tsx`)
â”œâ”€â”€ VersionMiniStepper     # 2-phase indicator
â”œâ”€â”€ Status Cards Row
â”‚   â”œâ”€â”€ VersionIngestionCard   # Ingestion stats & controls
â”‚   â””â”€â”€ VersionProcessingCard  # Processing stats & scan progress
â””â”€â”€ VersionDashboard       # KPI Cards & Charts
    â”œâ”€â”€ KPI Cards (Builds, Enriched, Quality, Features)
    â”œâ”€â”€ Build Status Bar
    â””â”€â”€ Top Issues List
â”œâ”€â”€ AnalysisSection        # Feature analysis & Scan Metrics
â”‚   â”œâ”€â”€ Quality Scores (Completeness, Validity, etc.)
â”‚   â”œâ”€â”€ Scan Metrics (Trivy/SonarQube results)
â”‚   â”œâ”€â”€ Feature distribution charts
â”‚   â”œâ”€â”€ Correlation matrix
â”‚   â””â”€â”€ Statistics overview

â””â”€â”€ ExportSection          # Export configuration
    â”œâ”€â”€ Format selection (CSV/JSON)
    â”œâ”€â”€ Feature selection
    â””â”€â”€ [Export] button
```

### Version Builds Page

```
VersionBuildsPage (in builds/page.tsx)
â”œâ”€â”€ Tab Navigation
â”‚   â”œâ”€â”€ Ingestion tab â†’ /builds/ingestion
â”‚   â”œâ”€â”€ Processing tab â†’ /builds/processing
â”‚   â””â”€â”€ Scans tab â†’ /builds/scans
â”‚       â”œâ”€â”€ Tabs: SonarQube, Trivy
â”‚       â””â”€â”€ Components: ScanTable (Commit, Status, Builds, Duration, Actions)

â””â”€â”€ Content Area
    â”œâ”€â”€ IngestionBuildsTable (per-build ingestion status)
    â”‚   â”œâ”€â”€ Build info (ID, repo, commit)
    â”‚   â”œâ”€â”€ Resource status (git_history, git_worktree, build_logs)
    â”‚   â””â”€â”€ Final status (ingested, missing_resource, failed)
    â””â”€â”€ ProcessingBuildsTable (per-build extraction status)
        â”œâ”€â”€ Build info
        â”œâ”€â”€ Extraction status
        â”œâ”€â”€ Feature count
        â””â”€â”€ Scan metrics status
```

### Key Differences from Model Pipeline UI

| Aspect | Model Pipeline (repositories) | Dataset Enrichment (projects) |
|--------|------------------------------|-------------------------------|
| Entry Point | Import GitHub repos | Upload CSV file |
| Stepper Phases | 4 phases (Fetch, Ingest, Extract, Predict) | 2 phases (Ingestion, Processing) |
| Prediction | Yes (ML model) | No (feature extraction only) |
| Scan Metrics | No | Yes (Trivy, SonarQube) |
| Feature Analysis | No | Yes (distribution, correlation) |
| Export | Basic | Advanced (preprocessing, format options) |

---

## Entities & Data Model

### Core Entities

#### DatasetVersion
```python
{
    _id: ObjectId,
    dataset_id: ObjectId,
    user_id: ObjectId,
    version_number: int,
    name: str,
    description: str,
    
    # Feature selection
    selected_features: List[str],
    scan_metrics: {
        "sonarqube": List[str],
        "trivy": List[str],
    },
    scan_config: {
        "sonarqube": Dict,
        "trivy": Dict,
    },
    
    # Status & progress (aligned with ModelRepoConfig)
    status: "queued|ingesting|ingested|processing|processed|failed",
    builds_total: int,              # Total builds to process
    builds_ingested: int,           # Successfully ingested builds
    builds_missing_resource: int,   # Builds with missing resources (not retryable)
    builds_ingestion_failed: int,   # Builds that failed ingestion (retryable)
    builds_features_extracted: int,   # Successfully extracted builds
    builds_extraction_failed: int,    # Failed during feature extraction
    # Timestamps
    started_at: datetime,
    completed_at: datetime,
    error_message: str,
    task_id: str,
}
```

#### DatasetImportBuild
```python
{
    _id: ObjectId,
    dataset_version_id: ObjectId,
    dataset_build_id: ObjectId,
    raw_repo_id: ObjectId,
    raw_build_run_id: ObjectId,
    
    # Status
    status: "pending|ingesting|ingested|missing_resource|failed",
    resource_status: {
        "git_history": {"status": "...", "error": "..."},
        "git_worktree": {...},
        "build_logs": {...},
    },
    required_resources: List[str],
    
    # Error tracking
    ingestion_error: str,  # Detailed error message
    
    # Denormalized fields
    ci_run_id: str,
    commit_sha: str,
    repo_full_name: str,
    
    # Tracking
    created_at: datetime,
    updated_at: datetime,
}
```

#### DatasetEnrichmentBuild
```python
{
    _id: ObjectId,
    dataset_version_id: ObjectId,
    dataset_build_id: ObjectId,
    dataset_id: ObjectId,
    raw_repo_id: ObjectId,
    raw_build_run_id: ObjectId,
    
    # ** FEATURE VECTOR REFERENCE (single source of truth) **
    feature_vector_id: ObjectId,  # Points to FeatureVector
    # All features stored in FeatureVector.features
    # All scan metrics stored in FeatureVector.scan_metrics
    
    # Status (mirrored from FeatureVector for quick queries)
    extraction_status: "pending|completed|partial|failed",
    extraction_error: str,
    enriched_at: datetime,
    
    # Tracking
    created_at: datetime,
    updated_at: datetime,
}
```

#### FeatureVector
```python
{
    _id: ObjectId,
    raw_repo_id: ObjectId,
    raw_build_run_id: ObjectId,  # 1:1 relationship (unique)
    
    # Version tracking
    dag_version: str,
    computed_at: datetime,
    
    # Temporal feature chain
    tr_prev_build: str,  # CI run ID of previous build
    
    # Status
    extraction_status: "pending|completed|partial|failed",
    extraction_error: str,
    
    # Graceful degradation
    is_missing_commit: bool,
    missing_resources: List[str],  # ["git_worktree", "build_logs"]
    skipped_features: List[str],   # Features skipped due to missing resources
    
    # ** FEATURES - Hamilton DAG extracted features **
    features: {
        "build_duration_ms": 1250,
        "build_logs_errors_count": 3,
        "git_diff_lines_added": 125,
        "git_diff_lines_deleted": 45,
        "tr_build_history_failure_rate": 0.15,
        "gh_author_commit_count": 42,
        ...
    },
    feature_count: int,
    
    # ** SCAN METRICS - Backfilled asynchronously **
    scan_metrics: {
        "trivy_vuln_total": 42,
        "trivy_vuln_critical": 3,
        "trivy_vuln_high": 8,
        "sonar_bugs": 5,
        "sonar_code_smells": 12,
        "sonar_coverage": 75.5,
        ...
    },
    
    # ** NORMALIZED FEATURES - For model prediction **
    normalized_features: {
        "build_duration_ms_scaled": 0.75,
        "git_diff_lines_scaled": 1.2,
        ...
    },
    
    # Tracking
    created_at: datetime,
    updated_at: datetime,
}
```

#### TrivyCommitScan
```python
{
    _id: ObjectId,
    dataset_version_id: ObjectId,
    commit_sha: str,
    repo_full_name: str,
    raw_repo_id: ObjectId,
    
    # Status
    status: "pending|scanning|completed|failed",
    error_message: str,
    
    # Config
    scan_config: Dict,
    selected_metrics: List[str],
    
    # Results
    metrics: Dict,
    builds_affected: int,
    
    # Tracking
    started_at: datetime,
    completed_at: datetime,
    created_at: datetime,
    retry_count: int,
}
```

#### SonarCommitScan
```python
{
    _id: ObjectId,
    dataset_version_id: ObjectId,
    commit_sha: str,
    repo_full_name: str,
    raw_repo_id: ObjectId,
    component_key: str,  # SonarQube project key
    
    # Status
    status: "pending|scanning|completed|failed",
    error_message: str,
    
    # Results
    metrics: Dict,
    builds_affected: int,
    
    # Tracking
    started_at: datetime,
    completed_at: datetime,
    created_at: datetime,
    retry_count: int,
}
```

### Repositories

Key repositories for enrichment:
- **DatasetVersionRepository**: Version tracking
- **DatasetImportBuildRepository**: Ingestion tracking
- **DatasetEnrichmentBuildRepository**: Processing tracking
- **FeatureVectorRepository**: Feature storage (single source of truth)
- **TrivyCommitScanRepository**: Trivy scan tracking
- **SonarCommitScanRepository**: SonarQube scan tracking
- **FeatureAuditLogRepository**: Audit trail

---

## Error Handling & Recovery

### Phase 1: Validation Errors

| Error | Handling | Recovery |
|-------|----------|----------|
| Repo not found on GitHub | Mark DatasetBuild.status = "not_found" | User reviews, removes from CSV |
| Build not found on CI | Mark DatasetBuild.status = "not_found" | User reviews, removes from CSV |
| Build filtered (doesn't match filter) | Mark DatasetBuild.status = "filtered" | User adjusts filters |
| CI API rate limit | Retry with exponential backoff | Automatic retry |
| GitHub API timeout | Retry with exponential backoff | Automatic retry |

### Phase 2: Ingestion Errors

**Error Classification:**
- **FAILED**: Actual error (timeout, network, exception) - **Retryable**
- **MISSING_RESOURCE**: Expected condition (logs expired 90+ days) - **Not retryable**

| Error | Status | Retryable | Recovery |
|------|--------|-----------|----------|
| Clone timeout | FAILED | âœ… | Automatic retry, then `reingest_failed_builds` |
| Worktree creation fail | FAILED | âœ… | `reingest_failed_builds` |
| Log download failed | FAILED | âœ… | `reingest_failed_builds` |
| Logs expired (90+ days) | MISSING_RESOURCE | âŒ | Cannot retry - logs permanently unavailable |
| Fork commit replay fail | MISSING_RESOURCE | âŒ | User can retry if fork is updated |
| Chord failure (worker crash) | FAILED | âœ… | `reingest_failed_builds` |

**Chord Error Callback**:
```
If ANY task in chord fails:
    â”œâ”€ Mark all IN_PROGRESS builds as FAILED (retryable)
    â”œâ”€ Store error details (ingestion_error)
    â”œâ”€ Check if any builds made it to INGESTED
    â”œâ”€ If yes â†’ Version status = INGESTED (can proceed)
    â”œâ”€ If no â†’ Version status = FAILED (cannot proceed)
    â””â”€ Publish error event
```

### Phase 3: Processing Errors

| Error | Handling | Recovery |
|------|----------|----------|
| Feature extraction fails | Mark build as FAILED | reprocess_failed_enrichment_builds |
| Trivy scan timeout | Retry up to 2 times | Automatic retry, builds marked missing trivy metrics |
| SonarQube scan fails | Mark SonarCommitScan as FAILED | User can retry scans manually |
| Temporal feature calculation error | Skip that feature | Mark as partial (feature_count reduced) |
| Chain failure (worker crash) | handle_enrichment_processing_chain_error | Mark IN_PROGRESS as FAILED |

**Chain Error Callback**:
```
If ANY task in chain fails:
    â”œâ”€ Mark all IN_PROGRESS builds as FAILED
    â”œâ”€ Count completed vs failed
    â”œâ”€ Update version status
    â”œâ”€ Publish error event
    â””â”€ User can retry with reprocess_failed_enrichment_builds
```

### Retry Strategies

**Exponential Backoff**:
```python
countdown = min(60 * (2**retry_count), max_countdown)
```

**Max Retries by Task**:
- clone_repo: 3 retries
- create_worktree_chunk: 2 retries (fork replay is complex)
- download_logs_chunk: 2 retries
- start_trivy_scan: 2 retries
- start_sonar_scan: 2 retries

**User-Triggered Retries**:
- `reingest_failed_builds` - Retry FAILED ingestion builds (not MISSING_RESOURCE)
- `reprocess_failed_enrichment_builds` - Retry failed processing
- Manual scan retry via UI (for SonarQube)

---

## Performance Optimization

### 1. Chunking Strategy

**CSV Validation**:
```python
VALIDATION_REPOS_PER_CHUNK = 50
VALIDATION_BUILDS_PER_CHUNK = 100
```

**Worktree Creation**:
```python
INGESTION_WORKTREES_PER_CHUNK = 200  # Sequential chunks
```

**Log Download**:
```python
INGESTION_LOGS_PER_CHUNK = 500  # Parallel chunks
```

**Scan Dispatch**:
```python
SCAN_BUILDS_PER_QUERY = 1000
SCAN_COMMITS_PER_BATCH = 100
SCAN_BATCH_DELAY_SECONDS = 0.5
```

### 2. Caching

- **RawRepository**: Cache GitHub metadata (prevents re-fetching)
- **RawBuildRun**: Cache CI metadata (prevents re-fetching)
- **Git bare repos**: Reuse across versions
- **Git worktrees**: Reuse if commit already exists
- **Build logs**: Cache on disk (don't re-download)

### 3. Database Optimization

**Indexing**:
- DatasetImportBuild: (dataset_version_id, status)
- DatasetEnrichmentBuild: (dataset_version_id, extraction_status)
- FeatureVector: (build_id, version_id, category)
- TrivyCommitScan: (dataset_version_id, commit_sha)
- SonarCommitScan: (dataset_version_id, commit_sha)

**Bulk Operations**:
- `bulk_insert` for DatasetImportBuild creation
- `bulk_update` for status changes
- `batch_update` for per-resource status

### 4. Redis Optimization

- **RedisLock**: Prevent concurrent operations (clone, worktree)

> **Note**: Tracking káº¿t quáº£ (ingestion, enrichment) sá»­ dá»¥ng trá»±c tiáº¿p database queries Ä‘á»ƒ Ä‘áº£m báº£o data durability

### 5. Parallelization

**Validation Phase**:
- Repo chunks in parallel (group)
- Build chunks per repo in parallel (group)

**Ingestion Phase**:
- Repo chains in parallel (group of chains)
- Log chunks per repo in parallel (chord with aggregate callback)
- Worktree chunks per repo sequential (chain for ordering)

**Processing Phase**:
- Scans for commits in parallel (group)
- Builds sequential (chain for temporal features)

**Scan Phase**:
- Trivy scans in parallel (trivy_scan queue)
- SonarQube scans in parallel (sonar_scan queue)

### 6. Resource Utilization

**Celery Workers**:
- validation: 2-4 workers
- ingestion: 4-8 workers (heavy I/O)
- trivy_scan: 2-4 workers (CPU-intensive)
- sonar_scan: 2-4 workers (network wait)
- processing: 4-8 workers (I/O + CPU)

**Git Resources**:
- Bare repos shared across versions
- Worktrees ephemeral (can be cleaned up)
- Disk space: ~2-5GB per repo (depends on history)

**Memory**:
- Feature extraction: ~500MB per build
- Scan execution: ~1-2GB per container

---

## Summary

### Key Design Principles

1. **Graceful Degradation**: Partial features better than no features
2. **Temporal Correctness**: Sequential processing for accurate temporal features
3. **Async Scans**: Don't block feature extraction
4. **Fault Tolerance**: Retry with exponential backoff, graceful fallbacks
5. **Auditability**: Track all operations in FeatureAuditLog
6. **Scalability**: Chunking, parallelization, caching
7. **Flexibility**: Dynamic resource calculation, per-repo config

### Processing Flow Summary

```
VALIDATION PHASE (validate repos & builds)
    â†“
INGESTION PHASE (clone, worktree, logs)
    â”œâ”€ Success: INGESTED
    â”œâ”€ Actual error: FAILED (retryable via reingest_failed_builds)
    â””â”€ Expected: MISSING_RESOURCE (not retryable - logs expired)
    â†“
PROCESSING PHASE (extract features)
    â”œâ”€ Async: Scan metrics (Trivy, SonarQube)
    â”œâ”€ Sequential: Feature extraction (temporal features)
    â”œâ”€ Backfill: Scan results to all builds with commit
    â””â”€ Auto: Quality evaluation on completion
    â†“
DATASET VERSION (enriched with features + scan metrics)
    â”œâ”€ Can export: CSV/JSON
    â””â”€ Can retry: reprocess_failed_enrichment_builds
```

---

**Last Updated**: December 31, 2025
**Version**: 1.0
